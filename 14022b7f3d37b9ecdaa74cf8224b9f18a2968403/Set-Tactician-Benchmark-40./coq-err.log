Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./theories/Relations/Operators_Properties.v", line 247, characters 6-18:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./theories/Structures/Orders.v", line 253, characters 1-8:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./theories/Structures/OrdersFacts.v", line 210, characters 51-57:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./theories/Classes/CMorphisms.v", line 615, characters 0-30:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./plugins/ssr/ssrfun.v", line 441, characters 48-66:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

File "./theories/Structures/OrdersFacts.v", line 223, characters 46-52:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

File "./plugins/ssr/ssrbool.v", line 877, characters 7-37:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./plugins/ssr/ssrbool.v", line 1814, characters 7-21:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

File "./plugins/ssr/ssrbool.v", line 505, characters 51-62:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

File "./plugins/ssr/ssrbool.v", line 1821, characters 7-27:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

File "./theories/Program/Wf.v", line 245, characters 4-33:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 211, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 191, in initialize_loop
    initialize_loop(r, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 182, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 139, in prediction_loop_text
    tactics = generate(g.predict.state.text, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/graph2tac/transformer/pserver.py", line 93, in generate
    input_ids = tokenizer([sample], truncate=True, max_length=900, return_tensors="pt", padding=False).input_ids.to(device)
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2517, in __call__
    return self.batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2708, in batch_encode_plus
    return self._batch_encode_plus(
  File "/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py", line 175, in _batch_encode_plus
    return super()._batch_encode_plus(*args, **kwargs)
TypeError: _batch_encode_plus() got an unexpected keyword argument 'truncate'
File "./theories/Numbers/Integer/Abstract/ZAddOrder.v", line 130, characters 0-7:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

Error:
Anomaly "Uncaught exception Unix.Unix_error(Unix.EPIPE, "single_write", "")."
Please report at http://coq.inria.fr/bugs/.

Abnormal exit code for coqc: 129
 Invocation:
OPAM_PACKAGE_NAME=coq-tactician-stdlib; export OPAM_PACKAGE_NAME;
CONDA_PROMPT_MODIFIER=(python3.9) ; export CONDA_PROMPT_MODIFIER;
TMUX=/tmp//tmux-50657/default,3013875,0; export TMUX;
LC_TIME=cs_CZ.UTF-8; export LC_TIME;
USER=piepejel; export USER;
LANGUAGE=en_US:en; export LANGUAGE;
SSH_CLIENT=10.35.1.2 54786 22; export SSH_CLIENT;
OPAMSWITCH=bench; export OPAMSWITCH;
SSH_AGENT_PID=3014379; export SSH_AGENT_PID;
SHLVL=2; export SHLVL;
HOME=/home/piepejel; export HOME;
CONDA_SHLVL=2; export CONDA_SHLVL;
OLDPWD=/raid/scratch/piepejel/projects; export OLDPWD;
MOTD_SHOWN=pam; export MOTD_SHOWN;
SSH_TTY=/dev/pts/4; export SSH_TTY;
CAML_LD_LIBRARY_PATH=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/stublibs:/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocaml/stublibs:/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocaml; export CAML_LD_LIBRARY_PATH;
OCAML_TOPLEVEL_PATH=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/toplevel; export OCAML_TOPLEVEL_PATH;
PS1=(venv) (python3.9) ${debian_chroot:+($debian_chroot)}\u@\h:\w\$ ; export PS1;
LC_MONETARY=cs_CZ.UTF-8; export LC_MONETARY;
ASYNC_PARALLEL_IS_CHILD_MACHINE=; export ASYNC_PARALLEL_IS_CHILD_MACHINE;
MAKEFLAGS= -j79 --jobserver-auth=3,4; export MAKEFLAGS;
_CE_M=; export _CE_M;
LOGNAME=piepejel; export LOGNAME;
_=/home/piepejel/.opam/tact/bin/tactician-benchmark; export _;
PKG_CONFIG_PATH=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/pkgconfig:/raid/scratch/piepejel/projects/coq-graph2tac-trained/_opam/lib/pkgconfig; export PKG_CONFIG_PATH;
OPAMROOT=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root; export OPAMROOT;
TERM=screen; export TERM;
_CE_CONDA=; export _CE_CONDA;
PATH=/raid/scratch/piepejel/projects/coq-graph2tac-trained/target-source/venv/bin:/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/bin:/raid/scratch/piepejel/projects/coq-graph2tac-trained/venv/bin:/usr/local/cuda/bin:/opt/bin:/home/piepejel/anaconda3/envs/python3.9/bin:/home/piepejel/anaconda3/condabin:/usr/local/cuda/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/piepejel/.local/bin:/home/piepejel/bin:/home/piepejel/.local/bin:/home/piepejel/bin; export PATH;
LC_ADDRESS=cs_CZ.UTF-8; export LC_ADDRESS;
MAKELEVEL=1; export MAKELEVEL;
CONDA_PREFIX_1=/home/piepejel/anaconda3; export CONDA_PREFIX_1;
LC_TELEPHONE=cs_CZ.UTF-8; export LC_TELEPHONE;
LANG=en_US.UTF-8; export LANG;
CDPATH=; export CDPATH;
SSH_AUTH_SOCK=/tmp/ssh-sdOla4DBmcbD/agent.3014378; export SSH_AUTH_SOCK;
CONDA_PYTHON_EXE=/home/piepejel/anaconda3/bin/python; export CONDA_PYTHON_EXE;
OPAMCLI=2.0; export OPAMCLI;
SHELL=/bin/bash; export SHELL;
LC_NAME=cs_CZ.UTF-8; export LC_NAME;
OPAM_PACKAGE_VERSION=8.11.dev; export OPAM_PACKAGE_VERSION;
CONDA_DEFAULT_ENV=python3.9; export CONDA_DEFAULT_ENV;
LC_MEASUREMENT=cs_CZ.UTF-8; export LC_MEASUREMENT;
OPAM_SWITCH_PREFIX=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench; export OPAM_SWITCH_PREFIX;
LC_IDENTIFICATION=cs_CZ.UTF-8; export LC_IDENTIFICATION;
VIRTUAL_ENV=/raid/scratch/piepejel/projects/coq-graph2tac-trained/venv; export VIRTUAL_ENV;
PWD=/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/.opam-switch/build/coq-tactician-stdlib.8.11.dev; export PWD;
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop; export XDG_DATA_DIRS;
SSH_CONNECTION=10.35.1.2 54786 10.35.64.30 22; export SSH_CONNECTION;
CONDA_EXE=/home/piepejel/anaconda3/bin/conda; export CONDA_EXE;
LC_NUMERIC=cs_CZ.UTF-8; export LC_NUMERIC;
MFLAGS=-j79 --jobserver-auth=3,4; export MFLAGS;
LC_PAPER=cs_CZ.UTF-8; export LC_PAPER;
CONDA_PREFIX=/home/piepejel/anaconda3/envs/python3.9; export CONDA_PREFIX;
TMUX_PANE=%0; export TMUX_PANE;
MANPATH=:/raid/scratch/piepejel/projects/coq-graph2tac-trained/_opam/man:/raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/man; export MANPATH;
(cd /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/.opam-switch/build/coq-tactician-stdlib.8.11.dev && bwrap --dev-bind / / --bind /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/.opam-switch/build/coq-tactician-stdlib.8.11.dev/theories/Relations/Operators_Properties.vo.bench /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/.opam-switch/build/coq-tactician-stdlib.8.11.dev/theories/Relations/Operators_Properties.vo /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/bin/coqc -q -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/coq/user-contrib/Tactician -R /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/coq/user-contrib/Tactician Tactician -rifrom Tactician Ltac1.Record -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/stdlib-shims -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocamlgraph -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocaml -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/lwt -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocplib-endian -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ocplib-endian/bigstring -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/lwt/unix -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/astring -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/res -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/result -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/stdint -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/asetmap -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/fmt -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/logs -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp-rpc -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/bigstringaf -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/angstrom -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/stringext -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/uri -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp-rpc-lwt -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/base64 -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/cstruct -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/eqaf -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/eqaf/bigstring -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/eqaf/cstruct -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto-rng -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-flow -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/re -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/prometheus -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ptime -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-clock -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/sexplib0 -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/zarith -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto-pk -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-kv -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/base/caml -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/parsexp -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/sexplib -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/cstruct-sexp -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/domain-name -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/hkdf -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/macaddr -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ipaddr -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ppx_sexp_conv/runtime-lib -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/ipaddr-sexp -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto-ec -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/asn1-combinators -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/gmap -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/pbkdf -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/x509 -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/tls -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/tls-mirage -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp-rpc-net -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/cmdliner -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/cstruct-lwt -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/extunix -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/duration -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto-rng/unix -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mtime -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mtime/clock/os -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/mirage-crypto-rng/lwt -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp-rpc-unix -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/base/base_internalhash_types -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/base/shadow_stdlib -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/base -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/stdio -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/capnp/unix -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/logs -l NNLearner -l Graph2TacConfig.v -l /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/share/coq-tactician/plugins/zzz-benchmark/Injections.v -q -coqlib . -I /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/coq/user-contrib/Tactician -R /raid/scratch/piepejel/projects/coq-graph2tac-trained/opam-root/bench/lib/coq/user-contrib/Tactician Tactician -rifrom Tactician Ltac1.Record theories/Relations/Operators_Properties.v -l /raid/scratch/piepejel/projects/coq-graph2tac-trained/BenchParams.v)