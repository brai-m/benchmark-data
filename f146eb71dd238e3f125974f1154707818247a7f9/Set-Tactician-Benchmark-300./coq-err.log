Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2023-05-30 15:56:46.650463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:48.024306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:48.038330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:48.325740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:49.213713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:51.871756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:54.962929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-30 15:56:58.365264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/bin/g2t-server-text", line 33, in <module>
    sys.exit(load_entry_point('graph2tac==0.1.0', 'console_scripts', 'g2t-server-text')())
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/graph2tac/transformer/pserver.py", line 276, in main
    initialize_loop(reader, s, textmode, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/graph2tac/transformer/pserver.py", line 247, in initialize_loop
    g = prediction_loop_text(r, s, tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/graph2tac/transformer/pserver.py", line 183, in prediction_loop_text
    tactics, probs = generate(g.predict.state.text.lstrip(), tokenizer, model)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/graph2tac/transformer/pserver.py", line 133, in generate
    beam_output = model.generate(
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1076, in forward
    transformer_outputs = self.transformer(
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 900, in forward
    outputs = block(
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 427, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 355, in forward
    hidden_states = self.act(hidden_states)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/scratch/piepejel/projects/coq-bench-15may/bd/target-source/venv/lib/python3.10/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 0; 31.75 GiB total capacity; 1.34 GiB already allocated; 218.75 MiB free; 1.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
File "./exercice_morley.v", line 121, characters 0-7:
Error: Anomaly "Capnp protocol error 3a"
Please report at http://coq.inria.fr/bugs/.

